---
layout: post  
title: 并行化卷积神经网络的一种奇妙技巧  
---

{{ page.title }}
================

<p class="meta">{{ page.date | date_to_string }} - Changsha</p>  

> _翻译自https://arxiv.org/abs/1404.5997_

+ model part：模型的一部分
+ batch size：批大小

+ 0.摘要  
我们提出一种新的跨多GPU的并行化卷积神经网络训练的方法。
当应用于现代卷积神经网络时，该方法比所有替代方案明显地可以更好地扩展(scales)。
+ 1.简介  
这是一个简短的说明，介绍了一种新的方法来以随机梯度下降（SGD）并行化卷积神经网络的训练。
我提出了该算法的两种变体。
第一个种完美地模拟了随机梯度下降法在单核上的同步执行，而第二种变体引入了近似，所以它就不是在完美地模拟随机梯度下降在单核上的同步执行，但是不管怎么样在实践中表现地很好。
+ 2.现有方法  
卷积神经网络是一个在大数据集上训练的大模型。所以明显有以下两种方法来并行化训练：  
  + 跨模型纬度，不同的workers训练模型的不同部分。
  + 跨数据纬度，不同的workers基于不同的数据样本训练。
这两种方式分别被称为模型并行和数据并行。  
在模型并行中，任何时候一个worker上训练的模型的一部分(神经元活动的子集)需要在其他worker上训练的模型的一部分的<font color="red">输出，两个worker必须同步<font>。
相比之下，在数据并行中，为了确保他们是在训练一个连续的模型，<font red="color">workers之间必须同步模型参数(参数梯度)<font>。  
通常，我们应该利用并行的所有纬度。
这两种方案都不比先前的方案好。
但是我们利用每个方案的相对程度应该通过模型架构来决定。
特别地，当每个神经元活动的计算量都很大的时候，模型并行是高效的(因为神经元活动是通信的单元)，而当每个权重当计算量很大的时候，数据并行是高效的(因为权重是通信单元)。  
影响所有这一切(all of this)的另一个因素是批大小。
如果我们愿意增加批大小(因为权重同步是每个批次执行一次)，我们可以使数据并行变得任意高效。
但是非常大的批大小对在该批次上的SGD的收敛速率和最终解决的质量有负面影响。
所以这里我把批大小设置为几百个或几千个样本。
+ 3.一些观察  
现代卷积神经网络由两种属性非常不同的层组成。
  + 卷积层累计大约90-95%的计算量，5%的参数，并且具有很大的表征性。
  + 全连接层大约有5-10%的计算量，95%的参数，表征能力较弱。
了解了这一点以后，我们就会很自然的想是否应该用不同的方法来并行这两种层。
特别地，数据并行适合卷积层而模型并行适合全连接层。  
这正是我所提出的方法。
在本文的内容中，我将更详细地解释该方案，并提到几个不错的属性。
+ 4.提出的算法
我提出的并行化训练卷积神经网络的方法，主要是在卷积层使用数据并行，在全连接层使用模型并行。
图1展示了K个workers上的该算法。  
![avatar](/images/posts/2019-03-14/parallelize-cnn-1.png)  
参考该图，前向传递的工作方式如下：
  + 1.K个workers中的每一个给以128个样本为一批的不同数据。
  + 2.K个workers中的每一个计算其批次上的所有的卷积层活动。
  + 3.为了计算全连接层的活动，workers切换到模型并行。这里有几种方式来完成模型并行：
    1. 每一个worker发送它自己最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    每个worker得到了一个批大小为128K个样本的大批并且在该批上如常地计算全连接层活动。
    2. workers中的一个worker发送它自己最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    每个worker开始计算在这个以128个样本上为一批的全连接活动然后为了这128个样本开始反向传播梯度(以下有更多关于这个的内容)。
    在并行中以这种方式计算，下一个worker继续发送它自己的最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    然后每个worker开始计算在这个以这第二个128个样本上为一批的全连接活动然后为了这128个样本开始反向传播梯度，然后依此类推。
    3. 所有的workers发送它自己最后一个阶段的卷积层的活动的128／K给其他的每一个worker(当然也会发送给当前worker)。
    这些workers然后按b中描述的方式处理。
    
    关于这三种方案的结果是很值得去思考的。  
    在方案a中，当每个worker在组装128K个图片的大批次的时候，所有有用的工作必须暂停。
    大的批次也会消耗大量的内存并且如果我们的workers是运行在内存有限制的设备上的话(比如GPU)，这样的大批次可能是不受欢迎的。
    另一方面，GPU通常能够更有效地在大批量上运行。  
    在方案b中，workers变成在广播它们最后一阶段的卷积层活动。
    这样主要的结果是很多(换句话说，K-1／K)的通信可以变成隐性的-它可以和全连接层的计算一起并行完成。
    这看起好很迷人，因为这是目前为止网络中最重要的通信。  
    方案c和方案b非常相似。
    它的一个优点是通信-计算比率是恒定的为K的。
    在方案a和b中，它与K成比例。
    这是因为方案a和方案b总是有worker的出去带宽的瓶颈，这样就得按给的的"步骤"来发送数据，而方案c能够利用所有的workers来完成这个任务。
    
  反向传播(backward pass)也是非常相似的：
  + 1.workers开始如常的计算全连接层的梯度
  + 2.接下来的步骤取决于前向传播的时候选择了3种方案中的哪一种：
    1. 在方案a中，每个worker已经为128K个样例的大批次计算了自己最后一个卷积层的活动梯度。
    <span style="border-bottom:2px solid red;">所以每个worker必须发送每个样例的梯度到在前向传播产生的那个样例的worker中去。</span>
    然后反向传播以正常的方式继续穿越卷积层。
    2. 在方案b中，每个worker已经为128个样例为一批的批次计算了自己最后一个卷积层的活动梯度。
    每个worker然后发送这些梯度给负责这128个样例为一批的的worker。
    于此同时(In parallel with this)，其他的workers在下一批128个样例上计算全连接前向传播。
    在K次这样穿过全连接层的前向和反向的迭代，workers一直传播梯度穿越整个卷积层。
    3. 方案c和方案b非常相似。
    每个worker已经为128个样例为一批的批次计算了自己最后一个卷积层的活动梯度。
    <span style="order-bottom:2px solid red;">这128个样例的批是每个worker贡献128/K个样例组装而成的，所以为了正确地发送梯度，我们必须逆操作。</span>
    剩下的流程和方案b一样。
    
    我再次指出，如同在前向传播中一样，方案c是三个方案中效率最高的，原因是一样的。
    
  方案b的K=2 workers的案例的前向和方向传播在图2中阐述。
  
+ 4.1 权重同步
