---
layout: post  
title: 并行化卷积神经网络的一种奇妙技巧  
---

{{ page.title }}
================

<p class="meta">{{ page.date | date_to_string }} - Changsha</p>  

> _翻译自https://arxiv.org/abs/1404.5997_

+ model part：模型的一部分
+ batch size：批大小

+ 0.摘要  
我们提出一种新的跨多GPU的并行化卷积神经网络训练的方法。
当应用于现代卷积神经网络时，该方法比所有替代方案明显地可以更好地扩展(scales)。
+ 1.简介  
这是一个简短的说明，介绍了一种新的方法来以随机梯度下降（SGD）并行化卷积神经网络的训练。
我提出了该算法的两种变体。
第一个种完美地模拟了随机梯度下降法在单核上的同步执行，而第二种变体引入了近似，所以它就不是在完美地模拟随机梯度下降在单核上的同步执行，但是不管怎么样在实践中表现地很好。
+ 2.现有方法  
卷积神经网络是一个在大数据集上训练的大模型。所以明显有以下两种方法来并行化训练：  
  + 跨模型纬度，不同的workers训练模型的不同部分。
  + 跨数据纬度，不同的workers基于不同的数据样本训练。
这两种方式分别被称为模型并行和数据并行。  
在模型并行中，任何时候一个worker上训练的模型的一部分(神经元活动的子集)需要在其他worker上训练的模型的一部分的<font color="red">输出，两个worker必须同步<font>。
相比之下，在数据并行中，为了确保他们是在训练一个连续的模型，<font red="color">workers之间必须同步模型参数(参数梯度)<font>。  
通常，我们应该利用并行的所有纬度。
这两种方案都不比先前的方案好。
但是我们利用每个方案的相对程度应该通过模型架构来决定。
特别地，当每个神经元活动的计算量都很大的时候，模型并行是高效的(因为神经元活动是通信的单元)，而当每个权重当计算量很大的时候，数据并行是高效的(因为权重是通信单元)。  
影响所有这一切(all of this)的另一个因素是批大小。
如果我们愿意增加批大小(因为权重同步是每个批次执行一次)，我们可以使数据并行变得任意高效。
但是非常大的批大小对在该批次上的SGD的收敛速率和最终解决的质量有负面影响。
所以这里我把批大小设置为几百个或几千个样本。
+ 3.一些观察  
现代卷积神经网络由两种属性非常不同的层组成。
  + 卷积层累计大约90-95%的计算量，5%的参数，并且具有很大的表征性。
  + 全连接层大约有5-10%的计算量，95%的参数，表征能力较弱。
了解了这一点以后，我们就会很自然的想是否应该用不同的方法来并行这两种层。
特别地，数据并行适合卷积层而模型并行适合全连接层。  
这正是我所提出的方法。
在本文的内容中，我将更详细地解释该方案，并提到几个不错的属性。
+ 4.提出的算法  
我提出的并行化训练卷积神经网络的方法，主要是在卷积层使用数据并行，在全连接层使用模型并行。
图1展示了K个workers上的该算法。  
![avatar](/images/posts/2019-03-14/parallelize-cnn-1.png)  
参考该图，前向传递的工作方式如下：
  + 1.K个workers中的每一个给以128个样本为一批的不同数据。
  + 2.K个workers中的每一个计算其批次上的所有的卷积层活动。
  + 3.为了计算全连接层的活动，workers切换到模型并行。这里有几种方式来完成模型并行：
    1. 每一个worker发送它自己最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    每个worker得到了一个批大小为128K个样本的大批并且在该批上如常地计算全连接层活动。
    2. workers中的一个worker发送它自己最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    每个worker开始计算在这个以128个样本上为一批的全连接活动然后为了这128个样本开始反向传播梯度(以下有更多关于这个的内容)。
    在并行中以这种方式计算，下一个worker继续发送它自己的最后一个阶段的卷积层的活动给其他的每一个worker(当然也会发送给当前worker)。
    然后每个worker开始计算在这个以这第二个128个样本上为一批的全连接活动然后为了这128个样本开始反向传播梯度，然后依此类推。
    3. 所有的workers发送它自己最后一个阶段的卷积层的活动的128／K给其他的每一个worker(当然也会发送给当前worker)。
    这些workers然后按b中描述的方式处理。
    
    关于这三种方案的结果是很值得去思考的。  
    在方案a中，当每个worker在组装128K个图片的大批次的时候，所有有用的工作必须暂停。
    大的批次也会消耗大量的内存并且如果我们的workers是运行在内存有限制的设备上的话(比如GPU)，这样的大批次可能是不受欢迎的。
    另一方面，GPU通常能够更有效地在大批量上运行。  
    在方案b中，workers变成在广播它们最后一阶段的卷积层活动。
    这样主要的结果是很多(换句话说，K-1／K)的通信可以变成隐性的-它可以和全连接层的计算一起并行完成。
    这看起好很迷人，因为这是目前为止网络中最重要的通信。  
    方案c和方案b非常相似。
    它的一个优点是通信-计算比率是恒定的为K的。
    在方案a和b中，它与K成比例。
    这是因为方案a和方案b总是有worker的出去带宽的瓶颈，这样就得按给的的"步骤"来发送数据，而方案c能够利用所有的workers来完成这个任务。
    
  反向传播(backward pass)也是非常相似的：
  + 1.workers开始如常的计算全连接层的梯度
  + 2.接下来的步骤取决于前向传播的时候选择了3种方案中的哪一种：
    1. 在方案a中，每个worker已经为128K个样例的大批次计算了自己最后一个卷积层的活动梯度。
    <span style="border-bottom:2px solid red;">所以每个worker必须发送每个样例的梯度到在前向传播产生的那个样例的worker中去。</span>
    然后反向传播以正常的方式继续穿越卷积层。
    2. 在方案b中，每个worker已经为128个样例为一批的批次计算了自己最后一个卷积层的活动梯度。
    每个worker然后发送这些梯度给负责这128个样例为一批的的worker。
    于此同时(In parallel with this)，其他的workers在下一批128个样例上计算全连接前向传播。
    在K次这样穿过全连接层的前向和反向的迭代，workers一直传播梯度穿越整个卷积层。
    3. 方案c和方案b非常相似。
    每个worker已经为128个样例为一批的批次计算了自己最后一个卷积层的活动梯度。
    <span style="order-bottom:2px solid red;">这128个样例的批是每个worker贡献128/K个样例组装而成的，所以为了正确地发送梯度，我们必须逆操作。</span>
    剩下的流程和方案b一样。
    
    我再次指出，如同在前向传播中一样，方案c是三个方案中效率最高的，原因是一样的。
    
  方案b的K=2 workers的案例的前向和方向传播在图2中阐述。  
  ![avatar](/images/posts/2019-03-14/parallelize-cnn-2.png)
  
+ 4.1 权重同步  
<span style="order-bottom:2px solid red;">一旦反向传播完成，workers便可更新权重。</span>
在卷积层中，workers之间也必须同步权重(或权重梯度)。
我能想到的完成权重同步的最简单的方法如下：
  + 1.每个worker被指定去同步梯度矩阵的K分之一。
  + 2.每个worker从每个其他worker累积相应的梯度的K分之一。
  + 3.每个worker广播这个已经累积好的梯度的K分之一给其他每一个worker。
这个步骤很容易实现，因为卷积权重很少。
+ 4.2 可变的批大小  
相比于标准的前向-反向传播，在方案b和c中我们所做的只是一个小小的改动，这个改动是完全等于以128K的批大小运行同步的SGD。
另请注意，方案（b）和（c）执行K次向前和向后传播穿过全连接层，每一次都是带着不同的128个样例的批。
<span style="border-bottom:2px solid red;">这意味着我们可以，如果我们想的话，在每个这些部分反向传播之后更新全连接层的权重，几乎没有额外的成本。</span>
我们可以把这认为是在全连接层使用批大小为128的批，在全连接层使用128K的批大小。
使用这种变化的批大小，该算法不再是一个纯粹的SGD并行化，因为在卷积层它不再为任何连续模型计算一个梯度更新。
<span style="border-bottom:2px solid red;">但是该方法在实践中效果不明显。因为我们把有效的批大小，128K，变成了千的级别，在全连接层使用更小的批大小可以更快地收敛到更好的最小值。</span>
+ 实验  
第一个我研究的问题是更大的批大小的准确性成本。
某种程度上讲，这是一个复杂的问题，因为答案是依数据集而定的。
相较于大的、异质的、有噪音的数据集而言，更小的批大小更有利于小的、相对同质的数据集。
这里，我报告在广泛使用的ImageNet 2012比赛数据集(ILSVRC 2012)上的实验。
在1.2百万张1000种类别的图片中，数据集规模处于某两种极端之间。
数据集规模不是很小，但是也不是“互联网规模”。
使用当前的GPUs(和CPUs)，在训练模型的时候，我们能负担迭代整个数据集很多次。  
我所考虑的模型是ILSVRC 2012比赛获胜模型的一个小变体。
最主要的不同是模型由一个“塔”组成，而不是两个“塔”。
该模型比双“塔”模型多了0.2%的参数并且少了2.4%的连接。
该模型和双“塔”模型的层数相同，每一层的(x, y)图维度和双“塔”模型相同。
在参数和连接方面的小小的不同来自一个在卷积层上卷积核数量的必要调整，因为在单“塔”模型里层与层间的连接没有限制。
其他的不同之处在于该模型最后一层有1000个独立的逻辑单元，这些独立的单元被训练以最小化交叉熵(cross-entropy)，而不是带多项逻辑回归损失的soft-max层。
<span style="border-bottom:2px solid red;">这个损失函数(CROSS-ENTROPY)性能和多项逻辑回归一样，但是它更容易并行，因为它不需要一个跨类别的正则化。</sapn>
我训练所有模型90轮，并且在训练进程到25%，50%和75%的时候将学习率乘以250<sup>-1/3</sup>。
